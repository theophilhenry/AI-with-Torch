{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bce53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Hyper-parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 100\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "writer = SummaryWriter('runs/mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb79db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = next(examples)\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "# for i in range(6):\n",
    "#   plt.subplot(2, 3, i+1)\n",
    "#   plt.imshow(samples[i][0], cmap='gray')\n",
    "# plt.show()\n",
    "img_grid = torchvision.utils.make_grid(samples)\n",
    "writer.add_image('mnist_images', img_grid)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05cfee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "  def forward(self, x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out\n",
    "  \n",
    "model = NeuralNet(input_size, hidden_size, num_classes)\n",
    "writer.add_graph(model, samples.reshape(-1, 28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258d32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb1dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "600\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_loader))[0].shape)\n",
    "print(len(train_loader)) # The number of batches in the train_loader\n",
    "# Total sample is\n",
    "total_samples = len(train_loader.dataset)\n",
    "print(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f76f4c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.4258\n",
      "Epoch [1/5], Step [200/600], Loss: 0.3194\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2558\n",
      "Epoch [1/5], Step [400/600], Loss: 0.4943\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1080\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1907\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1883\n",
      "Epoch [2/5], Step [200/600], Loss: 0.2891\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1842\n",
      "Epoch [2/5], Step [400/600], Loss: 0.2441\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1567\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2557\n",
      "Epoch [3/5], Step [100/600], Loss: 0.1703\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1135\n",
      "Epoch [3/5], Step [300/600], Loss: 0.1262\n",
      "Epoch [3/5], Step [400/600], Loss: 0.1228\n",
      "Epoch [3/5], Step [500/600], Loss: 0.1405\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0948\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0650\n",
      "Epoch [4/5], Step [200/600], Loss: 0.1767\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0415\n",
      "Epoch [4/5], Step [400/600], Loss: 0.1506\n",
      "Epoch [4/5], Step [500/600], Loss: 0.1270\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0343\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0720\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0788\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0527\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0707\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1225\n",
      "Epoch [5/5], Step [600/600], Loss: 0.1177\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "n_total_steps = len(train_loader)\n",
    "running_loss = 0.0\n",
    "running_correct = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for i, (images, labels) in enumerate(train_loader, 0):\n",
    "    # origin shape: [100, 1, 28, 28]\n",
    "    # resized: [100, 784]\n",
    "    images = images.reshape(-1, 28*28).to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    running_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    if (i+1) % 100 == 0:\n",
    "      # 100 step meaning for 10000 samples\n",
    "      print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "      writer.add_scalar('training loss', running_loss / 100, epoch * n_total_steps + i)\n",
    "      writer.add_scalar('training accuracy', running_correct / 100, epoch * n_total_steps + i)\n",
    "      running_loss = 0.0\n",
    "      running_correct = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75929c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97.14 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "real_predictions = []\n",
    "probability_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "  n_correct = 0\n",
    "  n_samples = 0\n",
    "  for images, labels in test_loader: # 100 samples / batch\n",
    "    images = images.reshape(-1, 28*28).to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    batched_raw_predictions = model(images)\n",
    "    batched_probability_predictions = [F.softmax(o, dim=0) for o in batched_raw_predictions]\n",
    "\n",
    "    # max returns (value ,index)\n",
    "    _, batched_real_predictions = torch.max(batched_raw_predictions.data, 1)\n",
    "    \n",
    "    probability_predictions.append(batched_probability_predictions)\n",
    "    real_predictions.append(batched_real_predictions)\n",
    "    \n",
    "    n_correct += (batched_real_predictions == labels).sum().item()\n",
    "    n_samples += labels.shape[0] # Can also just use len(test_loader.dataset)\n",
    "\n",
    "  real_predictions = torch.cat(real_predictions) # 10000\n",
    "  probability_predictions = torch.cat([torch.stack(batch) for batch in probability_predictions]) # 10000 x 10\n",
    "  \n",
    "  for i in range(10):\n",
    "    label_i = (real_predictions == i)\n",
    "    pred_i = probability_predictions[:, i]\n",
    "    writer.add_pr_curve(str(i), label_i, pred_i, global_step=0)\n",
    "    writer.close()\n",
    "  \n",
    "  acc = 100.0 * n_correct / n_samples\n",
    "  print(f'Accuracy of the network on the 10000 test images: {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00009966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "])\n",
    "a[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8e8018cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = a * 3\n",
    "c = b + 1\n",
    "c.backward()  # Computes gradients for 'a'\n",
    "print(b.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eefc78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.grad = 1.0\n",
      "w2.grad = 2.0\n",
      "w3.grad = 2.0\n",
      "w4.grad = 4.0\n",
      "w5.grad = 1.100000023841858\n",
      "w6.grad = 1.600000023841858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "# Input features (not trainable)\n",
    "# 2 features\n",
    "\n",
    "# 2, 4, 6\n",
    "# 4, 8, 12\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Weights for layer 1 (trainable)\n",
    "w1 = torch.tensor(0.5, requires_grad=True)\n",
    "w2 = torch.tensor(0.3, requires_grad=True)\n",
    "w3 = torch.tensor(0.2, requires_grad=True)\n",
    "w4 = torch.tensor(0.7, requires_grad=True)\n",
    "\n",
    "# Weights for layer 2 (trainable)\n",
    "w5 = torch.tensor(1.0, requires_grad=True)\n",
    "w6 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Hidden layer\n",
    "h1 = x[0]*w1 + x[1]*w2\n",
    "h2 = x[0]*w3 + x[1]*w4\n",
    "\n",
    "# (optional) Add an activation\n",
    "h1_act = torch.relu(h1)\n",
    "h2_act = torch.relu(h2)\n",
    "\n",
    "# Output layer\n",
    "y = h1_act * w5 + h2_act * w6\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Show gradients\n",
    "for name, param in zip(['w1','w2','w3','w4','w5','w6'], [w1,w2,w3,w4,w5,w6]):\n",
    "    print(f\"{name}.grad = {param.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "223c9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.grad = 1.0\n",
      "w2.grad = 2.0\n",
      "w3.grad = 2.0\n",
      "w4.grad = 4.0\n",
      "b1.grad = 1.0\n",
      "b2.grad = 2.0\n",
      "w5.grad = 1.2000000476837158\n",
      "w6.grad = 1.7000000476837158\n",
      "b3.grad = 1.0\n"
     ]
    }
   ],
   "source": [
    "# With Bias\n",
    "import torch\n",
    "\n",
    "# Input features (not trainable)\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Weights for layer 1 (trainable)\n",
    "w1 = torch.tensor(0.5, requires_grad=True)\n",
    "w2 = torch.tensor(0.3, requires_grad=True)\n",
    "w3 = torch.tensor(0.2, requires_grad=True)\n",
    "w4 = torch.tensor(0.7, requires_grad=True)\n",
    "\n",
    "# Bias terms for layer 1 (trainable)\n",
    "b1 = torch.tensor(0.1, requires_grad=True) # Bias for h1\n",
    "b2 = torch.tensor(0.1, requires_grad=True) # Bias for h2\n",
    "\n",
    "# Weights for layer 2 (trainable)\n",
    "w5 = torch.tensor(1.0, requires_grad=True)\n",
    "w6 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Bias term for layer 2 (output) (trainable)\n",
    "b3 = torch.tensor(3.0, requires_grad=True) # Bias for the output neuron (y)\n",
    "\n",
    "# -----------------\n",
    "# Hidden layer (Weighted Sum + Bias)\n",
    "# Bias b1 is added to h1\n",
    "h1 = x[0]*w1 + x[1]*w2 + b1 \n",
    "# Bias b2 is added to h2\n",
    "h2 = x[0]*w3 + x[1]*w4 + b2\n",
    "\n",
    "# (optional) Add an activation\n",
    "h1_act = torch.relu(h1)\n",
    "h2_act = torch.relu(h2)\n",
    "\n",
    "# Output layer (Weighted Sum + Bias)\n",
    "# Bias b3 is added to the final output sum\n",
    "y = h1_act * w5 + h2_act * w6 + b3\n",
    "\n",
    "# Backward pass\n",
    "y.backward()\n",
    "\n",
    "# Show gradients\n",
    "all_params = [w1, w2, w3, w4, b1, b2, w5, w6, b3]\n",
    "all_names = ['w1','w2','w3','w4', 'b1', 'b2', 'w5','w6', 'b3']\n",
    "\n",
    "for name, param in zip(all_names, all_params):\n",
    "    print(f\"{name}.grad = {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f48927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
