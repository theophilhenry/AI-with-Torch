{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "071e0358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100: Total reward = 1.00\n",
      "Episode 200: Total reward = -3.00\n",
      "Episode 300: Total reward = -16.00\n",
      "Episode 400: Total reward = -15.00\n",
      "Episode 500: Total reward = -46.00\n",
      "Episode 600: Total reward = -13.00\n",
      "Episode 700: Total reward = 10.00\n",
      "Episode 800: Total reward = -18.00\n",
      "Episode 900: Total reward = 13.00\n",
      "Episode 1000: Total reward = -52.00\n",
      "Episode 1100: Total reward = -3.00\n",
      "Episode 1200: Total reward = 0.00\n",
      "Episode 1300: Total reward = 1.00\n",
      "Episode 1400: Total reward = -11.00\n",
      "Episode 1500: Total reward = -9.00\n",
      "Episode 1600: Total reward = -27.00\n",
      "Episode 1700: Total reward = -5.00\n",
      "Episode 1800: Total reward = 14.00\n",
      "Episode 1900: Total reward = 12.00\n",
      "Episode 2000: Total reward = 3.00\n",
      "Episode 2100: Total reward = -2.00\n",
      "Episode 2200: Total reward = 19.00\n",
      "Episode 2300: Total reward = 6.00\n",
      "Episode 2400: Total reward = 23.00\n",
      "Episode 2500: Total reward = -16.00\n",
      "Episode 2600: Total reward = -21.00\n",
      "Episode 2700: Total reward = 1.00\n",
      "Episode 2800: Total reward = -7.00\n",
      "Episode 2900: Total reward = -20.00\n",
      "Episode 3000: Total reward = -16.00\n",
      "Episode 3100: Total reward = -25.00\n",
      "Episode 3200: Total reward = -37.00\n",
      "Episode 3300: Total reward = -14.00\n",
      "Episode 3400: Total reward = -18.00\n",
      "Episode 3500: Total reward = -12.00\n",
      "Episode 3600: Total reward = -46.00\n",
      "Episode 3700: Total reward = 18.00\n",
      "Episode 3800: Total reward = -27.00\n",
      "Episode 3900: Total reward = 21.00\n",
      "Episode 4000: Total reward = 11.00\n",
      "Episode 4100: Total reward = -4.00\n",
      "Episode 4200: Total reward = -4.00\n",
      "Episode 4300: Total reward = 38.00\n",
      "Episode 4400: Total reward = -9.00\n",
      "Episode 4500: Total reward = 19.00\n",
      "Episode 4600: Total reward = -20.00\n",
      "Episode 4700: Total reward = -15.00\n",
      "Episode 4800: Total reward = 19.00\n",
      "Episode 4900: Total reward = -22.00\n",
      "Episode 5000: Total reward = 31.00\n",
      "Episode 5100: Total reward = -9.00\n",
      "Episode 5200: Total reward = -2.00\n",
      "Episode 5300: Total reward = -9.00\n",
      "Episode 5400: Total reward = -14.00\n",
      "Episode 5500: Total reward = -24.00\n",
      "Episode 5600: Total reward = -3.00\n",
      "Episode 5700: Total reward = 17.00\n",
      "Episode 5800: Total reward = 11.00\n",
      "Episode 5900: Total reward = -7.00\n",
      "Episode 6000: Total reward = 0.00\n",
      "Episode 6100: Total reward = -25.00\n",
      "Episode 6200: Total reward = 11.00\n",
      "Episode 6300: Total reward = 3.00\n",
      "Episode 6400: Total reward = 19.00\n",
      "Episode 6500: Total reward = 5.00\n",
      "Episode 6600: Total reward = -11.00\n",
      "Episode 6700: Total reward = 13.00\n",
      "Episode 6800: Total reward = -17.00\n",
      "Episode 6900: Total reward = 13.00\n",
      "Episode 7000: Total reward = 16.00\n",
      "Episode 7100: Total reward = -13.00\n",
      "Episode 7200: Total reward = -45.00\n",
      "Episode 7300: Total reward = -26.00\n",
      "Episode 7400: Total reward = -32.00\n",
      "Episode 7500: Total reward = -41.00\n",
      "Episode 7600: Total reward = 9.00\n",
      "Episode 7700: Total reward = -25.00\n",
      "Episode 7800: Total reward = 17.00\n",
      "Episode 7900: Total reward = -21.00\n",
      "Episode 8000: Total reward = -7.00\n",
      "Episode 8100: Total reward = 18.00\n",
      "Episode 8200: Total reward = 3.00\n",
      "Episode 8300: Total reward = -13.00\n",
      "Episode 8400: Total reward = -22.00\n",
      "Episode 8500: Total reward = -4.00\n",
      "Episode 8600: Total reward = 7.00\n",
      "Episode 8700: Total reward = -9.00\n",
      "Episode 8800: Total reward = -26.00\n",
      "Episode 8900: Total reward = -3.00\n",
      "Episode 9000: Total reward = -3.00\n",
      "Episode 9100: Total reward = 0.00\n",
      "Episode 9200: Total reward = -14.00\n",
      "Episode 9300: Total reward = -12.00\n",
      "Episode 9400: Total reward = -25.00\n",
      "Episode 9500: Total reward = 25.00\n",
      "Episode 9600: Total reward = 10.00\n",
      "Episode 9700: Total reward = -30.00\n",
      "Episode 9800: Total reward = -42.00\n",
      "Episode 9900: Total reward = -26.00\n",
      "Episode 10000: Total reward = -15.00\n",
      "Training complete ✅\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "# Define a simple environment\n",
    "class InventoryEnv:\n",
    "    def __init__(self, max_inventory=10, max_demand=5):\n",
    "        self.max_inventory = max_inventory\n",
    "        self.max_demand = max_demand\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.inventory = random.randint(0, self.max_inventory)\n",
    "        self.demand = random.randint(0, self.max_demand)\n",
    "        return torch.tensor([self.inventory, self.demand], dtype=torch.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        # action = order amount (0, 1, 2)\n",
    "        self.inventory += action\n",
    "\n",
    "        # compute reward\n",
    "        reward = 0\n",
    "        if self.inventory >= self.demand:\n",
    "            reward = self.demand * 2 - (self.inventory - self.demand)  # reward high demand match, penalize overstock\n",
    "        else:\n",
    "            reward = - (self.demand - self.inventory) * 2  # penalty for shortage\n",
    "\n",
    "        # update inventory\n",
    "        self.inventory -= self.demand\n",
    "        self.inventory = max(0, min(self.inventory, self.max_inventory))\n",
    "\n",
    "        # new demand\n",
    "        self.demand = random.randint(0, self.max_demand)\n",
    "\n",
    "        next_state = torch.tensor([self.inventory, self.demand], dtype=torch.float32)\n",
    "        done = False\n",
    "        return next_state, torch.tensor(reward, dtype=torch.float32), done, {}\n",
    "\n",
    "# Q-Network\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self, state_size=2, action_size=3, hidden_size=32):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Train RL agent\n",
    "env = InventoryEnv()\n",
    "qnet = QNet()\n",
    "optimizer = optim.Adam(qnet.parameters(), lr=0.01)\n",
    "gamma = 0.9\n",
    "epsilon = 0.2\n",
    "num_episodes = 10000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(20):  # each episode = 20 steps\n",
    "        # Epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.randint(0, 2)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                q_values = qnet(state)\n",
    "                action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward.item()\n",
    "\n",
    "        # Compute target\n",
    "        with torch.no_grad():\n",
    "            target = reward + gamma * torch.max(qnet(next_state))\n",
    "\n",
    "        # Compute loss\n",
    "        pred = qnet(state)[action]\n",
    "        loss = (pred - target) ** 2\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode+1}: Total reward = {total_reward:.2f}\")\n",
    "\n",
    "print(\"Training complete ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d58a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing trained policy ---\n",
      "Step 1: State=[4. 2.] | Action=0 | Reward=-2.00\n",
      "Step 2: State=[4. 4.] | Action=2 | Reward=0.00\n",
      "Step 3: State=[2. 2.] | Action=2 | Reward=6.00\n",
      "Step 4: State=[2. 0.] | Action=2 | Reward=2.00\n",
      "Step 5: State=[3. 4.] | Action=1 | Reward=-3.00\n",
      "Step 6: State=[1. 4.] | Action=2 | Reward=7.00\n",
      "Step 7: State=[0. 1.] | Action=2 | Reward=-2.00\n",
      "Step 8: State=[1. 4.] | Action=2 | Reward=1.00\n",
      "Step 9: State=[0. 4.] | Action=2 | Reward=-2.00\n",
      "Step 10: State=[0. 0.] | Action=2 | Reward=-4.00\n",
      "Episode 1 total reward: 3.00\n",
      "Step 1: State=[0. 0.] | Action=2 | Reward=6.00\n",
      "Step 2: State=[2. 1.] | Action=2 | Reward=-2.00\n",
      "Step 3: State=[3. 2.] | Action=2 | Reward=-1.00\n",
      "Step 4: State=[3. 3.] | Action=2 | Reward=1.00\n",
      "Step 5: State=[2. 0.] | Action=2 | Reward=4.00\n",
      "Step 6: State=[3. 4.] | Action=1 | Reward=-3.00\n",
      "Step 7: State=[1. 5.] | Action=2 | Reward=7.00\n",
      "Step 8: State=[0. 1.] | Action=2 | Reward=-4.00\n",
      "Step 9: State=[1. 3.] | Action=2 | Reward=1.00\n",
      "Step 10: State=[0. 4.] | Action=2 | Reward=6.00\n",
      "Episode 2 total reward: 15.00\n",
      "Step 1: State=[6. 1.] | Action=2 | Reward=2.00\n",
      "Step 2: State=[5. 4.] | Action=0 | Reward=-3.00\n",
      "Step 3: State=[3. 2.] | Action=2 | Reward=5.00\n",
      "Step 4: State=[3. 3.] | Action=2 | Reward=1.00\n",
      "Step 5: State=[2. 2.] | Action=2 | Reward=4.00\n",
      "Step 6: State=[2. 1.] | Action=2 | Reward=2.00\n",
      "Step 7: State=[3. 0.] | Action=2 | Reward=-1.00\n",
      "Step 8: State=[3. 5.] | Action=0 | Reward=-3.00\n",
      "Step 9: State=[0. 2.] | Action=2 | Reward=10.00\n",
      "Step 10: State=[0. 2.] | Action=2 | Reward=4.00\n",
      "Episode 3 total reward: 21.00\n",
      "Step 1: State=[0. 4.] | Action=2 | Reward=10.00\n",
      "Step 2: State=[0. 2.] | Action=2 | Reward=-4.00\n",
      "Step 3: State=[0. 5.] | Action=2 | Reward=4.00\n",
      "Step 4: State=[0. 5.] | Action=2 | Reward=-6.00\n",
      "Step 5: State=[0. 4.] | Action=2 | Reward=-6.00\n",
      "Step 6: State=[0. 5.] | Action=2 | Reward=-4.00\n",
      "Step 7: State=[0. 4.] | Action=2 | Reward=-6.00\n",
      "Step 8: State=[0. 5.] | Action=2 | Reward=-4.00\n",
      "Step 9: State=[0. 2.] | Action=2 | Reward=-6.00\n",
      "Step 10: State=[0. 3.] | Action=2 | Reward=4.00\n",
      "Episode 4 total reward: -18.00\n",
      "Step 1: State=[6. 1.] | Action=0 | Reward=-6.00\n",
      "Step 2: State=[5. 3.] | Action=0 | Reward=-3.00\n",
      "Step 3: State=[4. 0.] | Action=2 | Reward=2.00\n",
      "Step 4: State=[4. 1.] | Action=0 | Reward=-4.00\n",
      "Step 5: State=[3. 0.] | Action=0 | Reward=-1.00\n",
      "Step 6: State=[3. 2.] | Action=0 | Reward=-3.00\n",
      "Step 7: State=[3. 4.] | Action=2 | Reward=1.00\n",
      "Step 8: State=[1. 0.] | Action=2 | Reward=7.00\n",
      "Step 9: State=[2. 2.] | Action=1 | Reward=-2.00\n",
      "Step 10: State=[2. 3.] | Action=2 | Reward=2.00\n",
      "Episode 5 total reward: -7.00\n",
      "\n",
      "Average test reward: 2.80\n"
     ]
    }
   ],
   "source": [
    "# ---- TESTING PHASE ----\n",
    "print(\"\\n--- Testing trained policy ---\")\n",
    "\n",
    "test_env = InventoryEnv()\n",
    "total_test_reward = 0\n",
    "\n",
    "for episode in range(5):  # run 5 test episodes\n",
    "    state = test_env.reset()\n",
    "    episode_reward = 0\n",
    "\n",
    "    for t in range(10):  # each episode = 10 steps\n",
    "        with torch.no_grad():\n",
    "            q_values = qnet(state)\n",
    "            action = torch.argmax(q_values).item()  # always pick best action\n",
    "\n",
    "        next_state, reward, done, _ = test_env.step(action)\n",
    "        episode_reward += reward.item()\n",
    "        state = next_state\n",
    "\n",
    "        print(f\"Step {t+1}: State={state.numpy()} | Action={action} | Reward={reward.item():.2f}\")\n",
    "\n",
    "    print(f\"Episode {episode+1} total reward: {episode_reward:.2f}\")\n",
    "    total_test_reward += episode_reward\n",
    "\n",
    "print(f\"\\nAverage test reward: {total_test_reward / 5:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3680b650",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
